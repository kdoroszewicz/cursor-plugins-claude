---
description: Best practices for MongoDB queries, aggregation pipelines, indexing, transactions, and performance optimization
globs: "**/*.ts", "**/*.js"
---

# MongoDB Query Best Practices

## Create Indexes for Query Patterns

- **Every query should be backed by an index** — queries without indexes perform full collection scans.
- Create indexes that match your `find()` filter, sort, and projection fields (the **ESR rule**: Equality → Sort → Range).
- Use **compound indexes** for queries that filter and sort on multiple fields.
- Use `explain('executionStats')` to verify index usage.

```typescript
// Create indexes that match your query patterns
// If you query: db.orders.find({ status: 'pending', customerId: id }).sort({ createdAt: -1 })
// Create this compound index:
db.collection('orders').createIndex(
  { status: 1, customerId: 1, createdAt: -1 },
  { name: 'idx_orders_status_customer_date' }
);

// Mongoose: define indexes on the schema
orderSchema.index({ status: 1, customerId: 1, createdAt: -1 });

// Verify index usage
const explanation = await Order.find({ status: 'pending' })
  .sort({ createdAt: -1 })
  .explain('executionStats');
console.log(explanation.executionStats.executionStages.stage); // should be IXSCAN, not COLLSCAN
```

## Use Projection to Limit Fields

- **Always project only the fields you need** — transferring unnecessary data wastes bandwidth and memory.
- A projection can also leverage a **covered query** (index-only query) if all requested fields are in the index.

```typescript
// BAD: fetching all fields when you only need name and email
const users = await User.find({ active: true });

// GOOD: project only needed fields
const users = await User.find({ active: true })
  .select('name email')
  .lean();

// Native driver
const users = await db.collection('users')
  .find({ active: true }, { projection: { name: 1, email: 1, _id: 0 } })
  .toArray();
```

## Use Aggregation Pipeline for Complex Transformations

- Prefer the **aggregation pipeline** over application-side data processing for grouping, reshaping, and computing.
- Order stages for performance: `$match` and `$project` early to reduce the working set.
- Use `$lookup` for cross-collection joins — but keep them minimal and indexed.

```typescript
// Aggregation: monthly revenue report
const report = await Order.aggregate([
  // 1. Filter early to reduce working set
  { $match: {
    status: 'delivered',
    createdAt: { $gte: startOfYear, $lte: endOfYear },
  }},
  // 2. Group by month
  { $group: {
    _id: { $dateToString: { format: '%Y-%m', date: '$createdAt' } },
    totalRevenue: { $sum: { $toDecimal: '$total' } },
    orderCount: { $sum: 1 },
    avgOrderValue: { $avg: { $toDecimal: '$total' } },
  }},
  // 3. Sort by month
  { $sort: { _id: 1 } },
  // 4. Reshape output
  { $project: {
    month: '$_id',
    totalRevenue: { $round: ['$totalRevenue', 2] },
    orderCount: 1,
    avgOrderValue: { $round: ['$avgOrderValue', 2] },
    _id: 0,
  }},
]);
```

```typescript
// $lookup: join orders with customer details
const enrichedOrders = await Order.aggregate([
  { $match: { status: 'pending' } },
  { $lookup: {
    from: 'customers',
    localField: 'customerId',
    foreignField: '_id',
    as: 'customer',
    pipeline: [
      { $project: { name: 1, email: 1, tier: 1 } },
    ],
  }},
  { $unwind: '$customer' },
]);
```

## Handle Cursor-Based Pagination

- **Avoid `skip()` for large datasets** — `skip(N)` forces MongoDB to scan and discard N documents on every query.
- Use **cursor-based pagination** (also called keyset pagination) for consistent performance at any page depth.
- Paginate using a unique, indexed field (e.g., `_id`, `createdAt` combined with `_id`).

```typescript
// BAD: skip-based pagination — degrades at high offsets
const page = await Post.find({ published: true })
  .sort({ createdAt: -1 })
  .skip((pageNumber - 1) * pageSize)
  .limit(pageSize);

// GOOD: cursor-based pagination — consistent performance
async function getNextPage(lastId?: string, pageSize = 20) {
  const query: any = { published: true };
  if (lastId) {
    query._id = { $lt: new Types.ObjectId(lastId) };
  }
  const results = await Post.find(query)
    .sort({ _id: -1 })
    .limit(pageSize + 1)  // fetch one extra to detect if there's a next page
    .lean();

  const hasNextPage = results.length > pageSize;
  const items = hasNextPage ? results.slice(0, -1) : results;
  const nextCursor = hasNextPage ? items[items.length - 1]._id.toString() : null;

  return { items, nextCursor, hasNextPage };
}
```

## Use bulkWrite for Batch Operations

- Use **`bulkWrite()`** for batch inserts, updates, and deletes — it sends all operations in a single round-trip.
- Prefer `ordered: false` when operations are independent — it allows MongoDB to execute them in parallel.

```typescript
// BAD: individual operations in a loop
for (const item of items) {
  await Product.updateOne(
    { sku: item.sku },
    { $set: { price: item.price, updatedAt: new Date() } },
    { upsert: true }
  );
}

// GOOD: batch all operations in a single bulkWrite
const operations = items.map(item => ({
  updateOne: {
    filter: { sku: item.sku },
    update: { $set: { price: item.price, updatedAt: new Date() } },
    upsert: true,
  },
}));

const result = await Product.bulkWrite(operations, { ordered: false });
console.log(`Matched: ${result.matchedCount}, Upserted: ${result.upsertedCount}`);
```

## Handle Duplicate Key Errors

- Always handle **`E11000` duplicate key errors** gracefully — they occur on unique index violations.
- Use `upsert` when appropriate to avoid separate find-then-insert patterns.
- Catch and handle the error with a clear message.

```typescript
import { MongoServerError } from 'mongodb';

async function createUser(data: CreateUserInput) {
  try {
    const user = await User.create(data);
    return user;
  } catch (error) {
    if (error instanceof MongoServerError && error.code === 11000) {
      const field = Object.keys(error.keyPattern)[0];
      throw new ConflictError(`A user with that ${field} already exists`);
    }
    throw error;
  }
}

// Alternative: use findOneAndUpdate with upsert
async function upsertUser(email: string, data: Partial<UserInput>) {
  return User.findOneAndUpdate(
    { email },
    { $set: data, $setOnInsert: { createdAt: new Date() } },
    { upsert: true, new: true, runValidators: true }
  );
}
```

## Use Transactions for Multi-Document Atomicity

- Use **transactions** when you need atomicity across multiple documents or collections.
- Transactions require a **replica set** or sharded cluster (not standalone).
- Keep transactions **short** — long-running transactions hold locks and degrade performance.
- Always handle `TransientTransactionError` and `UnknownTransactionCommitResult` with retry logic.

```typescript
// Mongoose transaction: transfer funds between accounts
async function transferFunds(fromId: string, toId: string, amount: number) {
  const session = await mongoose.startSession();
  try {
    session.startTransaction({
      readConcern: { level: 'snapshot' },
      writeConcern: { w: 'majority' },
    });

    const from = await Account.findOneAndUpdate(
      { _id: fromId, balance: { $gte: amount } },
      { $inc: { balance: -amount } },
      { session, new: true }
    );

    if (!from) {
      throw new Error('Insufficient funds or account not found');
    }

    await Account.findOneAndUpdate(
      { _id: toId },
      { $inc: { balance: amount } },
      { session }
    );

    await TransactionLog.create([{
      from: fromId,
      to: toId,
      amount,
      type: 'transfer',
      timestamp: new Date(),
    }], { session });

    await session.commitTransaction();
  } catch (error) {
    await session.abortTransaction();
    throw error;
  } finally {
    session.endSession();
  }
}
```

## Use Change Streams for Reactive Patterns

- Use **change streams** to react to real-time data changes without polling.
- Change streams require a replica set or sharded cluster.
- Always store and use **resume tokens** for fault-tolerant stream consumption.

```typescript
// Watch for new orders and trigger processing
const pipeline = [
  { $match: {
    operationType: { $in: ['insert', 'update'] },
    'fullDocument.status': 'pending',
  }},
];

const changeStream = Order.watch(pipeline, { fullDocument: 'updateLookup' });

changeStream.on('change', async (change) => {
  console.log(`New/updated order: ${change.fullDocument._id}`);
  await processOrder(change.fullDocument);
});

changeStream.on('error', (error) => {
  console.error('Change stream error:', error);
  // Reconnect using the resume token
  const resumeToken = changeStream.resumeToken;
  // restart stream with { resumeAfter: resumeToken }
});
```

## Optimize with explain()

- Use **`explain('executionStats')`** to understand query performance.
- Look for: `COLLSCAN` (bad — no index), `IXSCAN` (good — using index), `totalDocsExamined` vs `nReturned`.
- The goal: `totalDocsExamined` should be close to `nReturned`.

```typescript
// Check if your query uses an index
const plan = await User.find({ email: 'alice@example.com' })
  .explain('executionStats');

const stats = plan.executionStats;
console.log({
  executionTimeMs: stats.executionTimeMillis,
  totalDocsExamined: stats.totalDocsExamined,
  totalKeysExamined: stats.totalKeysExamined,
  nReturned: stats.nReturned,
  indexUsed: stats.executionStages.inputStage?.indexName ?? 'NONE (COLLSCAN)',
});
```

## Set Read/Write Concerns Appropriately

- **Write concern** — controls durability guarantee:
  - `w: 1` (default) — acknowledged by the primary.
  - `w: 'majority'` — acknowledged by a majority of replica set members (safer).
  - `w: 0` — fire-and-forget (fastest, but no error detection).
  - `j: true` — wait for journal commit (protects against power loss).
- **Read concern** — controls consistency of reads:
  - `local` (default) — returns the most recent data available on the node.
  - `majority` — returns data that has been acknowledged by a majority (avoids reading rollback-prone data).
  - `snapshot` — for transactions; provides a consistent point-in-time view.
- **Read preference** — controls which node to read from:
  - `primary` (default) — always read from primary.
  - `primaryPreferred` — read from primary, fallback to secondary.
  - `secondary` — read from secondaries (for read scaling or analytics).
  - `nearest` — read from the lowest-latency node.

```typescript
// Configure per-operation
const user = await User.findOne({ email })
  .read('secondaryPreferred')
  .lean();

// Configure at connection level
mongoose.connect(uri, {
  readPreference: 'secondaryPreferred',
  w: 'majority',
  journal: true,
  readConcern: { level: 'majority' },
});
```

## Native Driver vs Mongoose

### Use the Native Driver When

- You need **maximum performance** and minimal overhead.
- You are working with **raw aggregation pipelines** or database-level operations.
- You want fine-grained control over every operation.
- You are building **infrastructure or tooling** rather than application code.

```typescript
import { MongoClient } from 'mongodb';

const client = new MongoClient(uri);
const db = client.db('myapp');
const users = db.collection('users');

// Direct, no overhead
const result = await users.findOne({ email: 'alice@example.com' });
```

### Use Mongoose When

- You want **schema validation** and type safety at the application level.
- You need **middleware** (pre/post hooks), **virtuals**, **getters/setters**, and **population**.
- You value **developer experience** — Mongoose's API is more expressive for CRUD-heavy apps.
- You are building a **typical web application** with well-defined models.

```typescript
import mongoose, { Schema, model } from 'mongoose';

const userSchema = new Schema({
  email: { type: String, required: true, unique: true },
  name: String,
  role: { type: String, enum: ['user', 'admin'], default: 'user' },
}, { timestamps: true });

const User = model('User', userSchema);
const user = await User.findOne({ email: 'alice@example.com' });
```

## Connection Management

- Create **one `MongoClient` or `mongoose.connect()` call** for the entire application — never create connections per request.
- Use **connection pooling** (default pool size is 100 for the native driver, 5 for Mongoose).
- Handle connection errors and implement reconnection logic.
- Close the connection gracefully on application shutdown.

```typescript
// Mongoose: singleton connection
import mongoose from 'mongoose';

let isConnected = false;

export async function connectToDatabase() {
  if (isConnected) return;

  await mongoose.connect(process.env.MONGODB_URI!, {
    maxPoolSize: 10,
    minPoolSize: 2,
    serverSelectionTimeoutMS: 5000,
    socketTimeoutMS: 45000,
    retryWrites: true,
    w: 'majority',
  });

  isConnected = true;
  console.log('Connected to MongoDB');
}

// Graceful shutdown
process.on('SIGTERM', async () => {
  await mongoose.connection.close();
  console.log('MongoDB connection closed');
  process.exit(0);
});
```

## Error Handling

- Always handle MongoDB-specific errors with proper error codes.
- Implement **retry logic** for transient errors (network timeouts, elections).
- Never expose raw MongoDB errors to end users.

```typescript
import { MongoServerError, MongoNetworkError } from 'mongodb';

async function safeDbOperation<T>(operation: () => Promise<T>, retries = 3): Promise<T> {
  for (let attempt = 1; attempt <= retries; attempt++) {
    try {
      return await operation();
    } catch (error) {
      if (error instanceof MongoNetworkError && attempt < retries) {
        const delay = Math.pow(2, attempt) * 100;
        console.warn(`Transient error, retrying in ${delay}ms (attempt ${attempt}/${retries})`);
        await new Promise(resolve => setTimeout(resolve, delay));
        continue;
      }
      if (error instanceof MongoServerError) {
        switch (error.code) {
          case 11000: throw new ConflictError('Duplicate key violation');
          case 121:   throw new ValidationError('Document failed validation');
          default:    throw new DatabaseError(`Database error: ${error.message}`);
        }
      }
      throw error;
    }
  }
  throw new Error('Exhausted retries');
}
```
