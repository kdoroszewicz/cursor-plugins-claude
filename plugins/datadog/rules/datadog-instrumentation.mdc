---
description: Best practices for Datadog APM instrumentation, distributed tracing, custom spans, and unified service tagging.
globs:
  - "**/*.ts"
  - "**/*.js"
  - "**/*.py"
alwaysApply: false
---

# Datadog APM & Instrumentation Best Practices

## Automatic Instrumentation with dd-trace

- Use `dd-trace` for automatic instrumentation of supported libraries and frameworks. Import and initialize the tracer **before** any other imports so that integrations are patched at load time.
  - In Node.js: create a `tracer.ts` (or `tracer.js`) file that initializes the tracer, and import it first in your entry point.
  - In Python: use `ddtrace-run` to auto-instrument your application at startup, or call `ddtrace.patch_all()` at the top of your entry module.

```ts
// tracer.ts — import this FIRST in your entry file
import tracer from "dd-trace";

tracer.init({
  service: "my-api",
  env: process.env.DD_ENV || "development",
  version: process.env.DD_VERSION || "1.0.0",
  logInjection: true,
  runtimeMetrics: true,
  profiling: true,
});

export default tracer;
```

```python
# For Python, prefer ddtrace-run:
# ddtrace-run python app.py

# Or initialize manually:
from ddtrace import tracer, patch_all

patch_all()
tracer.configure(
    hostname=os.environ.get("DD_AGENT_HOST", "localhost"),
    port=int(os.environ.get("DD_TRACE_AGENT_PORT", 8126)),
)
```

## Unified Service Tagging

- **Always** set the three unified service tags — `env`, `service`, and `version` — on every trace, log, and metric. This is the foundation of Datadog's correlation model.
- Set them via environment variables so they are consistent across all telemetry:

```bash
DD_ENV=production
DD_SERVICE=my-api
DD_VERSION=1.2.3
```

- The tracer picks these up automatically, but you can also set them explicitly in `tracer.init()`:

```ts
tracer.init({
  service: process.env.DD_SERVICE || "my-api",
  env: process.env.DD_ENV || "production",
  version: process.env.DD_VERSION || "1.0.0",
});
```

- Use consistent service names across all components of the same logical service. Do not vary service names by deployment or instance — use tags for that distinction.

## Custom Spans for Business Logic

- Add custom spans around important business operations to gain visibility into application-specific flows:

```ts
import tracer from "./tracer";

async function processOrder(order: Order): Promise<OrderResult> {
  return tracer.trace("order.process", { resource: `order-${order.type}` }, async (span) => {
    span.setTag("order.id", order.id);
    span.setTag("order.total", order.total);
    span.setTag("order.items_count", order.items.length);

    const validated = await tracer.trace("order.validate", async () => {
      return validateOrder(order);
    });

    const payment = await tracer.trace("payment.charge", { service: "payment-service" }, async (paymentSpan) => {
      paymentSpan.setTag("payment.method", order.paymentMethod);
      return chargePayment(order);
    });

    const result = await tracer.trace("order.fulfill", async () => {
      return fulfillOrder(order, payment);
    });

    return result;
  });
}
```

```python
from ddtrace import tracer

@tracer.wrap(service="order-service", resource="process_order")
def process_order(order):
    with tracer.trace("order.validate") as span:
        span.set_tag("order.id", order.id)
        validate_order(order)

    with tracer.trace("payment.charge") as span:
        span.set_tag("payment.method", order.payment_method)
        charge_payment(order)
```

## Meaningful Service and Resource Names

- **Service names** should represent a logical unit of work (e.g., `web-api`, `user-service`, `payment-gateway`). Do not include environment, hostname, or instance identifiers in the service name.
- **Resource names** should be parameterized, low-cardinality identifiers for the operation:
  - HTTP endpoints: `GET /api/users/:id` (not `GET /api/users/550e8400-e29b-41d4-a716-446655440000`)
  - Database queries: `SELECT users` (not the full SQL text with interpolated values)
  - Background jobs: `job.send_welcome_email`
- High-cardinality resource names fragment your APM data and make it impossible to aggregate and alert effectively.

## Tags for Filtering and Analysis

- Use **span tags** to add searchable metadata to traces for filtering in the Trace Explorer:

```ts
span.setTag("customer.tier", customer.tier);       // "free", "pro", "enterprise"
span.setTag("feature_flag.checkout_v2", isEnabled); // true / false
span.setTag("region", request.region);              // "us-east-1"
```

- Keep tag values low-cardinality — avoid UUIDs, timestamps, or unbounded strings as tag values.
- Use tags for dimensions you want to filter, group, or alert on in dashboards and monitors.
- Standard Datadog tags to always include: `env`, `service`, `version`, `team`, `component`.

## Sampling Rate Configuration

- Configure trace sampling to balance observability with cost:
  - **Development**: `DD_TRACE_SAMPLE_RATE=1.0` (capture everything)
  - **Staging**: `DD_TRACE_SAMPLE_RATE=0.5`
  - **Production**: Use Datadog Agent-level sampling or ingestion controls for fine-grained control.
- Use **sampling rules** for per-service or per-resource rate control:

```ts
tracer.init({
  ingestion: {
    sampleRate: 0.1, // default: sample 10%
    rateLimit: 100,  // max traces per second
  },
});
```

```bash
# Agent-level sampling rules (datadog.yaml or env var)
DD_APM_FILTER_TAGS_REQUIRE=env:production
DD_APM_FEATURES=enable_rare_sampler
```

- Use Datadog's **Ingestion Controls** page to monitor ingestion volumes and adjust sampling without redeploying.
- Always sample 100 % of error traces — configure the tracer to keep error spans regardless of the sampling rate:

```ts
tracer.init({
  sampler: {
    sampleRate: 0.1,
    rules: [
      { sampleRate: 1.0, service: "my-api", name: "express.request", tags: { "error": "1" } },
    ],
  },
});
```

## Distributed Trace Context Propagation

- Ensure trace context is propagated across all service boundaries — HTTP, gRPC, message queues, and async workers.
- `dd-trace` automatically propagates context for supported HTTP clients (`axios`, `fetch`, `http`, `requests`). For custom transports, manually inject/extract headers:

```ts
import tracer from "dd-trace";

// Inject context into outgoing request headers
const headers: Record<string, string> = {};
tracer.inject(tracer.scope().active()!, "http_headers", headers);
// Pass `headers` to your HTTP client or message envelope

// Extract context from incoming request
const spanContext = tracer.extract("http_headers", req.headers);
const span = tracer.startSpan("custom.operation", { childOf: spanContext! });
```

- For message queues (Kafka, SQS, RabbitMQ), embed trace headers in message attributes and extract them in the consumer.
- Verify propagation by checking that traces in the Datadog UI show the full distributed call chain across services.

## Trace–Log Correlation

- Enable log injection so that every log line includes `dd.trace_id` and `dd.span_id`:

```ts
tracer.init({
  logInjection: true, // Automatically injects trace IDs into supported loggers
});
```

- When using structured logging (pino, winston, bunyan), the trace and span IDs are added as fields automatically. Use these to jump from a log entry to the corresponding trace in Datadog:

```json
{
  "message": "Order processed successfully",
  "dd": {
    "trace_id": "1234567890abcdef",
    "span_id": "abcdef1234567890",
    "service": "my-api",
    "env": "production",
    "version": "1.2.3"
  }
}
```

- In Python, configure `ddtrace` log injection:

```python
import logging
from ddtrace import tracer, patch_all

patch_all()

# Standard logging — dd-trace injects trace IDs via log formatter
logging.basicConfig(
    format="%(asctime)s %(levelname)s [dd.service=%(dd.service)s dd.trace_id=%(dd.trace_id)s dd.span_id=%(dd.span_id)s] %(message)s"
)
```

## Handling Sensitive Data in Traces

- **Never** include PII, credentials, or secrets in span tags or resource names.
- Use `DD_APM_OBFUSCATION_*` environment variables or `blocklist` configuration to redact sensitive data:

```ts
tracer.init({
  tags: {
    // Do NOT do this:
    // "user.ssn": user.ssn,
    // "auth.token": token,
  },
});
```

- Redact sensitive query parameters from HTTP resource names:

```bash
# Agent-level obfuscation in datadog.yaml
apm_config:
  obfuscation:
    http:
      remove_query_string: true
      remove_paths_with_digits: false
    sql:
      obfuscate_sql_values: true
```

- Use the `beforeSend` / `hooks` feature of the tracer to scrub spans before they are sent:

```ts
tracer.init({
  hooks: {
    "http:start": (span) => {
      // Remove sensitive headers
      span.setTag("http.request.headers.authorization", undefined);
      span.setTag("http.request.headers.cookie", undefined);
    },
  },
});
```

## Error Tracking in Traces

- Mark spans as errors and attach error metadata for proper error tracking in Datadog:

```ts
try {
  await riskyOperation();
} catch (error) {
  const span = tracer.scope().active();
  if (span) {
    span.setTag("error", true);
    span.setTag("error.message", error.message);
    span.setTag("error.type", error.constructor.name);
    span.setTag("error.stack", error.stack);
  }
  throw error;
}
```

- Use Datadog Error Tracking to group, triage, and monitor errors across services. Error Tracking automatically deduplicates errors by stack trace fingerprint.

## Best Practices Summary

| Practice | Rationale |
|---|---|
| Initialize tracer before all other imports | Ensures automatic instrumentation patches all libraries |
| Set unified service tags (env, service, version) | Enables correlation across traces, logs, and metrics |
| Use parameterized, low-cardinality resource names | Prevents data fragmentation, enables aggregation |
| Add custom spans for business logic | Provides visibility into application-specific flows |
| Configure sampling per environment | Balances observability needs with ingestion costs |
| Propagate trace context across boundaries | Maintains distributed trace continuity |
| Enable log injection for correlation | Links logs to traces for faster debugging |
| Scrub PII and secrets from spans | Protects sensitive data and ensures compliance |
