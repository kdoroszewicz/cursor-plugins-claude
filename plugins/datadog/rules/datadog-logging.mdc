---
description: Best practices for Datadog log management — structured logging, trace correlation, log pipelines, and data hygiene.
globs:
  - "**/*.ts"
  - "**/*.js"
  - "**/*.py"
alwaysApply: false
---

# Datadog Logging Best Practices

## Structured JSON Logging

- **Always** use structured JSON logging instead of plain-text log lines. Datadog's log pipeline processes JSON natively and can automatically extract attributes, set facets, and apply processors.
- Use a structured logging library appropriate for your language:
  - **Node.js**: `pino`, `winston`, or `bunyan`
  - **Python**: `structlog`, `python-json-logger`, or the standard `logging` module with a JSON formatter

```ts
// Recommended: pino with JSON output
import pino from "pino";

const logger = pino({
  level: process.env.LOG_LEVEL || "info",
  formatters: {
    level(label) {
      return { level: label };
    },
  },
  timestamp: pino.stdTimeFunctions.isoTime,
  messageKey: "message",
});

logger.info({ orderId: "abc-123", total: 99.99 }, "Order processed successfully");
// Output: {"level":"info","time":"2026-01-15T10:30:00.000Z","message":"Order processed successfully","orderId":"abc-123","total":99.99}
```

```ts
// Alternative: winston with JSON format
import winston from "winston";

const logger = winston.createLogger({
  level: process.env.LOG_LEVEL || "info",
  format: winston.format.combine(
    winston.format.timestamp(),
    winston.format.errors({ stack: true }),
    winston.format.json()
  ),
  defaultMeta: {
    service: process.env.DD_SERVICE || "my-api",
    env: process.env.DD_ENV || "development",
    version: process.env.DD_VERSION || "1.0.0",
  },
  transports: [new winston.transports.Console()],
});
```

```python
# Python: structlog with JSON output
import structlog

structlog.configure(
    processors=[
        structlog.contextvars.merge_contextvars,
        structlog.processors.add_log_level,
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
        structlog.processors.JSONRenderer(),
    ],
    wrapper_class=structlog.make_filtering_bound_logger(logging.INFO),
)

logger = structlog.get_logger()
logger.info("order_processed", order_id="abc-123", total=99.99)
```

## Trace–Log Correlation with trace_id and span_id

- Include `dd.trace_id` and `dd.span_id` in every log line to enable seamless navigation between logs and traces in Datadog.
- With `dd-trace` and `logInjection: true`, supported loggers (pino, winston, bunyan) automatically include trace context:

```ts
import tracer from "dd-trace";

tracer.init({
  logInjection: true, // Automatically adds dd.trace_id, dd.span_id to logs
});
```

- Verify correlation by checking that your log entries contain `dd.trace_id` and `dd.span_id` fields:

```json
{
  "message": "Payment charged",
  "level": "info",
  "dd": {
    "trace_id": "5765030529756398734",
    "span_id": "1456789012345678",
    "service": "payment-api",
    "env": "production",
    "version": "2.1.0"
  },
  "payment_id": "pay_abc123",
  "amount": 49.99
}
```

- In Python, use the `ddtrace` log formatter or manually inject trace context:

```python
import logging
from ddtrace import tracer

# Automatic injection via ddtrace logging integration
from ddtrace.contrib.logging import patch as patch_logging
patch_logging()

FORMAT = (
    "%(asctime)s %(levelname)s [dd.service=%(dd.service)s "
    "dd.env=%(dd.env)s dd.version=%(dd.version)s "
    "dd.trace_id=%(dd.trace_id)s dd.span_id=%(dd.span_id)s] "
    "%(message)s"
)
logging.basicConfig(format=FORMAT, level=logging.INFO)
```

## Proper Log Levels

- Use log levels consistently across all services. Datadog maps these to status values for filtering and alerting:

| Level | Use For | Example |
|---|---|---|
| `debug` | Verbose diagnostic information (disable in production) | Variable values, execution flow |
| `info` | Normal operational events | Request received, order processed, job started |
| `warn` | Recoverable issues that may need attention | Retry attempt, deprecated API usage, high latency |
| `error` | Failures requiring investigation | Unhandled exception, payment failure, DB timeout |
| `fatal` / `critical` | System-level failures requiring immediate action | Process crash, OOM, data corruption |

- **Never** log at `error` level for expected conditions (e.g., user input validation failures). Use `warn` or `info` instead.
- **Never** log at `debug` level in production unless temporarily enabled for troubleshooting — debug logs generate excessive volume and cost.
- Set log level via environment variable (`LOG_LEVEL`) so you can adjust without redeploying.

## Meaningful Attributes and Context

- Add structured attributes to logs that enable effective searching, filtering, and alerting in Datadog:

```ts
logger.info({
  event: "order.created",
  order_id: order.id,
  customer_id: order.customerId,
  total: order.total,
  currency: order.currency,
  items_count: order.items.length,
  payment_method: order.paymentMethod,
  duration_ms: endTime - startTime,
}, "Order created successfully");
```

- Use **consistent attribute names** across services. Adopt naming conventions from Datadog's standard attributes:
  - `usr.id`, `usr.email`, `usr.name` for user fields
  - `http.method`, `http.url`, `http.status_code` for HTTP fields
  - `db.type`, `db.statement`, `db.instance` for database fields
  - `error.message`, `error.kind`, `error.stack` for error fields
  - `duration` (in nanoseconds) for timing measurements

- Avoid logging raw request/response bodies — log only the relevant summary attributes.

## Avoid Logging Sensitive Data

- **Never** log passwords, API keys, tokens, credit card numbers, SSNs, or other PII.
- Use an allowlist approach — explicitly choose which fields to log rather than logging entire objects:

```ts
// BAD — logs everything including sensitive fields
logger.info({ user }, "User logged in");

// GOOD — log only safe, relevant fields
logger.info({
  event: "user.login",
  user_id: user.id,
  email_domain: user.email.split("@")[1], // domain only, no full email
  login_method: "oauth",
}, "User logged in");
```

- Use Datadog's **Sensitive Data Scanner** to automatically detect and redact PII that may slip through:
  - Navigate to **Logs → Configuration → Sensitive Data Scanner**
  - Create scanning rules for credit cards, emails, API keys, etc.
  - Choose to redact, hash, or partially mask matched patterns

- Configure log pipeline processors to remove sensitive attributes before indexing:

```yaml
# In a Datadog log pipeline processor (Remapper / Attribute Remover)
# Remove sensitive headers from indexed logs
- type: attribute_remover
  name: Remove auth headers
  sources:
    - http.request.headers.authorization
    - http.request.headers.cookie
    - http.request.headers.x-api-key
```

## Log Pipelines and Processors

- Use **Log Pipelines** in Datadog to parse, enrich, and transform logs before indexing:
  - **Grok Parser**: Extract structured fields from unstructured log lines.
  - **Date Remapper**: Use the log's timestamp field instead of intake time.
  - **Status Remapper**: Map your log level field to Datadog's status.
  - **Service Remapper**: Map your service name field to Datadog's service attribute.
  - **Attribute Remapper**: Rename attributes to match standard naming conventions.
  - **Category Processor**: Add categorical tags based on log content.
  - **Lookup Processor**: Enrich logs with data from a reference table.

### Recommended Pipeline Setup

1. **Service & Source Detection** — Auto-detect `service` and `source` from log metadata or container tags.
2. **Date Remapping** — Remap `timestamp` or `@timestamp` to the official log date.
3. **Status Remapping** — Map `level` or `severity` to Datadog log status.
4. **Grok Parsing** — Parse any non-JSON log lines into structured attributes.
5. **Geo-IP Enrichment** — Enrich client IP addresses with geographic data.
6. **User-Agent Parsing** — Extract browser/OS from user-agent strings.
7. **Sensitive Data Redaction** — Scrub PII and secrets.

## Log Indexes and Retention

- Use **Log Indexes** to control which logs are retained and for how long:
  - Create separate indexes for different retention needs (e.g., `production-errors` with 30-day retention, `production-info` with 15-day retention).
  - Use index filters to route logs to the appropriate index based on `status`, `service`, `env`, or custom attributes.
  - Set daily quotas on indexes to prevent runaway costs from log spikes.

- Use **Log Archives** to send all logs to long-term storage (S3, GCS, Azure Blob) for compliance and historical analysis.

## Log Exclusion Filters

- Use **exclusion filters** on indexes to drop noisy, low-value logs before they consume your indexing quota:

```
# Exclude health check logs
service:my-api status:info @http.url:/health*

# Exclude verbose debug logs that were accidentally enabled
service:* level:debug

# Exclude bot traffic logs
service:web-frontend @http.useragent:*bot*
```

- Always review exclusion filters periodically — over-aggressive filtering can hide important signals.
- Use **Live Tail** to verify that exclusion filters are not dropping logs you need.

## Datadog Transports for Logging Libraries

### Pino with Datadog

```ts
import pino from "pino";

// Option 1: Log to stdout, Datadog Agent collects from container stdout
const logger = pino({
  level: "info",
  messageKey: "message",
});

// Option 2: Send logs directly to Datadog via HTTP (serverless / no Agent)
import { createWriteStream } from "pino-datadog-transport";

const transport = pino.transport({
  target: "pino-datadog-transport",
  options: {
    apiKey: process.env.DD_API_KEY,
    service: process.env.DD_SERVICE || "my-api",
    ddsource: "nodejs",
    ddtags: `env:${process.env.DD_ENV},version:${process.env.DD_VERSION}`,
  },
});

const logger = pino({ level: "info", messageKey: "message" }, transport);
```

### Winston with Datadog

```ts
import winston from "winston";

// Option 1: Console transport — Datadog Agent collects stdout
const logger = winston.createLogger({
  level: "info",
  format: winston.format.combine(
    winston.format.timestamp(),
    winston.format.json()
  ),
  transports: [new winston.transports.Console()],
});

// Option 2: Direct HTTP transport to Datadog
import { DatadogTransport } from "datadog-winston";

const logger = winston.createLogger({
  level: "info",
  format: winston.format.json(),
  transports: [
    new winston.transports.Console(),
    new DatadogTransport({
      apiKey: process.env.DD_API_KEY!,
      service: process.env.DD_SERVICE || "my-api",
      ddsource: "nodejs",
      ddtags: `env:${process.env.DD_ENV},version:${process.env.DD_VERSION}`,
    }),
  ],
});
```

### Bunyan with Datadog

```ts
import bunyan from "bunyan";
import DatadogStream from "bunyan-datadog-stream";

const logger = bunyan.createLogger({
  name: process.env.DD_SERVICE || "my-api",
  level: "info",
  streams: [
    { stream: process.stdout },
    {
      type: "raw",
      stream: new DatadogStream({
        apiKey: process.env.DD_API_KEY!,
        ddsource: "nodejs",
        service: process.env.DD_SERVICE || "my-api",
        ddtags: `env:${process.env.DD_ENV},version:${process.env.DD_VERSION}`,
      }),
    },
  ],
});
```

### Python logging with Datadog

```python
import logging
import json_log_formatter

# JSON formatter for structured logging
formatter = json_log_formatter.JSONFormatter()

handler = logging.StreamHandler()
handler.setFormatter(formatter)

logger = logging.getLogger("my_service")
logger.addHandler(handler)
logger.setLevel(logging.INFO)

# For direct HTTP shipping (serverless / no Agent):
from datadog_api_client import Configuration, ApiClient
from datadog_api_client.v2.api.logs_api import LogsApi

# Or use the datadog_logger handler:
# pip install datadog
from datadog import initialize, api
```

## Best Practices Summary

| Practice | Rationale |
|---|---|
| Use structured JSON logging | Enables automatic parsing, faceting, and searching in Datadog |
| Include trace_id and span_id | Links logs to traces for correlated debugging |
| Use consistent, appropriate log levels | Enables effective filtering, alerting, and cost control |
| Add meaningful structured attributes | Powers search, dashboards, and anomaly detection |
| Never log sensitive data | Protects PII and secrets; ensures compliance |
| Configure log pipelines | Normalizes and enriches logs for consistent analysis |
| Set index retention and quotas | Controls costs and ensures relevant data is available |
| Use exclusion filters for noisy logs | Reduces indexing costs without losing data in archives |
| Use Datadog-compatible transports | Ensures logs flow to Datadog with proper metadata |
